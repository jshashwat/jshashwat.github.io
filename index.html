<!DOCTYPE html>
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->
	<head>
		<meta charset="utf-8">
		<title>Computer Vision Course Project</title>
		<meta name="description" content="Fall 2021 ECE 4554/5554 Computer Vision: Course Project">
		<meta name="author" content="Shashwat Jain">

		<!-- Mobile Meta -->
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<!-- Favicon -->
		<link rel="shortcut icon" href="images/favicon.ico">
 
		<link href="bootstrap/css/bootstrap.css" rel="stylesheet"> 
		<link href="fonts/font-awesome/css/font-awesome.css" rel="stylesheet"> 
		<link href="css/animations.css" rel="stylesheet"> 
		<link href="css/style.css" rel="stylesheet"> 
		<link href="css/custom.css" rel="stylesheet">
	</head>

	<body class="no-trans">
		<!-- scrollToTop --> 
		<div class="scrollToTop"><i class="icon-up-open-big"></i></div>

		<!-- header start --> 
		<header class="header fixed clearfix navbar navbar-fixed-top">
			<div class="container">
				<div class="row">
					<div class="col-md-4">

						<!-- header-left start --> 
						<div class="header-left">

							<!-- logo -->
							<div class="logo smooth-scroll">
								<a href="#banner"><img id="logo" src="images/logo.png" alt="Worthy"></a>
							</div>

							<!-- name-and-slogan -->
							<div class="logo-section smooth-scroll">
								<div class="brand"><a href="#banner"><font size="+2">COMPUTER VISION</font></a></div>								
							</div>

						</div>
						<!-- header-left end -->

					</div>
					<div class="col-md-8">

						<!-- header-right start --> 
						<div class="header-right">

							<!-- main-navigation start --> 
							<div class="main-navigation animated">

								<!-- navbar start --> 
								<nav class="navbar navbar-default" role="navigation">
									<div class="container-fluid">

										<!-- Toggle get grouped for better mobile display -->
										<div class="navbar-header">
											<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-collapse-1">
												<span class="sr-only">Toggle navigation</span>
												<span class="icon-bar"></span>
												<span class="icon-bar"></span>
												<span class="icon-bar"></span>
											</button>
										</div>

										<!-- Collect the nav links, forms, and other content for toggling -->
										<div class="collapse navbar-collapse scrollspy smooth-scroll" id="navbar-collapse-1">
											<ul class="nav navbar-nav navbar-right">
												<li class="active"><a href="#banner">Home</a></li>
												<li><a href="#abstract">Abstract</a></li>
												<li><a href="#intro">Introduction</a></li>
												<li><a href="#teaser">Approach</a></li>
												
												<li><a href="#portfolio">Experiments and Results</a></li>
											

												<!--<li><a href="#contact">Contact</a></li> -->
											</ul>
										</div>

									</div>
								</nav>
								<!-- navbar end -->

							</div>
							<!-- main-navigation end -->

						</div>
						<!-- header-right end -->

					</div>
				</div>
			</div>
		</header>
		<!-- header end -->

		<!-- banner start --> 
		<div id="banner" class="banner">
			<div class="banner-image"></div>
			<div class="banner-caption">
				<div class="container">
					<div class="row">					 
						<div class="caption-data" style="margin-top: 0px; opacity: 1;" data-animation-effect="fadeIn">
								<h1>Throughput Calculator for Queues: <a target="_blank" href="https://learnopencv.com/histogram-of-oriented-gradients/">HoG</a> 
									vs <a target="_blank" href="https://pjreddie.com/darknet/yolo/">YOLO</a></h1>
								<h3 class="padding-top30">Kriti Kansal and Shashwat Jain<br/> Fall 2021 ECE 4554/5554 Computer Vision: Course Project<br/> Virginia Tech</h3>
								<!--<div class="padding-top60 contact-form">
									<button class="btn cta-button">CLICK HERE</button>
								</div> -->
							</div>
					</div>
				</div>
			</div>
		</div>
		<!-- banner end -->

		<!--
		<section class="hero-caption secPadding">

		<div class="container">
	
	<div class="row " style="margin-top: 0px;">
				<div class="col-sm-12">
	<h2><strong>ABSTRACT</strong></h2>
<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit. Iure aperiam consequatur quo. Sed quis tortor magna. Maecenas hendrerit feugiat pulvinar. Aenean condimentum quam eu ultricies cursus.  Nulla facilisi. In hac habitasse platea dictumst. Ut nec tellus neque. Sed non dui eget arcu elementum facilisis.</p>
 	</div>
	
			</div>

		</div>
	-->
</section>
<!-- section start --> 
		<section class="section transprant-bg pclear secPadding">
			<div class="container no-view" data-animation-effect="fadeIn">
				<h1 id="abstract" class="title text-center">Abstract</h1>
				<p class = "p_justify"> </p>

					
					
			<p class = "p_justify">There have been many advancements in technology that are making our devices smarter and smarter. Taking a small step in this direction, we have designed and developed a queue tracker.
				 Queue tracker calculates the throughput of queues in grocery stores, cafes, and airports etc. This project serves as a basis to study mainstream algorithms in computer vision that will help us to figure
				 out an optimum solution for real-time throughput calculation in queues.
				 In our approach, we have implemented the detection using two algorithms: HoG and YOLO. The aim of this project is to compare the characteristics of HoG and YOLO. Upon experimentation, 
				 we realised that HoG is must faster than YOLO, however, the accuracy is much better in YOLO.
			</p>
			</div>			  
			<br>
			<br>
			<div class="container no-view" data-animation-effect="fadeIn">
				<h1 id="intro" class="title text-center">Introduction</h1>
				<p class = "p_justify">Customer experience is one of the most important factors that define the success of any business. 
					In a busy world like ours, nobody wants to wait in long queues. 
					It can be just a coffee shop or a queue at the airport! As technology gets more advanced day by day, 
					it is becoming easier to install computer vision techniques at places like these to help in reducing the waiting time for the customers. 
					In this project, we will implement a computer vision model to calculate the throughput of queues in real-time. 
					This will enable us to dynamically direct the customers to the queues that have better throughput and less waiting time, efficiently managing the intra-space traffic. </p>

					
			<p class = "p_justify">We have compared two techniques for detecting people in queues and to calculate the throughput of queues: <a target="_blank" href="https://learnopencv.com/histogram-of-oriented-gradients/">HoG (Histogram of Oriented Gradients)</a> and 
				<a target="_blank" href="https://pjreddie.com/darknet/yolo/">YOLO (You Only Look Once)</a>, a Deep Learning model for object detection. 
				Our main goal was to compare the results of these two techniques for queue tracking applications. As expected YOLO worked very well with slow as well as fast-paced objects in video feeds but the per frame 
				execution time was higher as compared to HoG. We expected the results from HoG to be comparable because of relatively slow-paced movement of objects in video feeds for queueing applications, which turned out to be relatively true and we will discuss more
				on that in the experiments and results section. HoG had some aberrations in detection with intermittent false positives but YOLO seemed to be quite accurate and stable with the results.</p>
				<p class = "p_justify">Even though both HoG and YOLO are widely used in multiple applications, our aim is to compare these two in a different real-world applications.</p>
			</div>			  
		</section>

		<!-- section start --> 
		<section class="section clearfix no-view secPadding" data-animation-effect="fadeIn">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<h1 id="teaser" class="title text-center"> <span>Approach</span></h1>
							 
							 Here is the basic setup for the algorithms used in the framework.
							 <ul class="list-unstyled">
								<li><i class="fa fa-arrow-circle-right pr-10 colored"></i>Data Preprocessing</li>
									<ul>
										<li>Capture Frames from Input Feed: Our framework creates an OpenCV VideCapture object that is attached to an input source. The object constantly captures
											frames from the source video feed. The source video feed can be a prerecorded video of the queuing environment setup or a live feed (provide camera id as source input) 
											of a similar setup.</li>
										<li>Crop and Resize: We crop the input video feed to only focus on the area with the visible queue, to avoid multiple false positives due to people
											that might be present in the background. This process is for now hardcoded from the config file that our source code uses. This assumption is made due to the fact that the camera
											mounted for such usecase will be fixed and the whole setup will behave in a certain geometry. Thus, these cropping parameters can be provided during initial installation. We have resized
											the input frames to our detection algorithm to a max width of 500 pixels.</li>
										<li>Dataset Used: <a target="_blank" href="https://cvgl.stanford.edu/projects/collective/collectiveActivity.html">Collective Activity Dataset</a>
											: We will leverage Class ID 4 in the dataset as our Data for Queuing Application.</li>
									</ul>
							
								<li> <i class="fa fa-arrow-circle-right pr-10 colored"></i>HoG </li>
									<ul>
										<li>Parameters:</li>
											<ul>
												<li>win_stride: It defines a step size for the detector window to move in the x and y direction. The step size decides the amount of information to be processed in each window, 
													if the win_stride is small, we will be able to capture fine-grained details. We have chosen a win_stride of 4 to balance the time taken and information processed.
												</li>
												<li>Padding: We have padded the sliding window with 6 pixels. 
												</li>
												<li>Scale:Scale defines the factor by which the image is resized at each layer of a pyramid. 
													A smaller scale influences the number of layers in the image pyramid. 
													A smaller value will give us better results but will be computationally expensive. This is why we have chosen a scale of 1.01.
													</li>
											</ul>
									</ul>
		
								<li><i class="fa fa-arrow-circle-right pr-10 colored"></i>YOLO</li>
									<ul>
										<li>Parameters:</li>
											<ul>
												<li>Input Resolution: A large pixel resolution increases accuracy, 
													however it takes a longer time to predict. The pixel resolution should be a multiple of 32, in this project we have maintained a standard resolution size of 416x416.</li>
		
												<li>Pre-Trained Weights: Since our queue throughput calculator detects people to predict the throughput, it was an obvious choice to use pre-trained weights from COCO dataset. 
													Ideally, we would have wanted to train our own data set, however considering the scope of this project we chose to use the pre-trained weights. </li>
												
												<li>ConfThreshold and nmsThreshold: Bounding boxes with confidence value less than ConfThreshold are discarded. We have chosen a value of 0.4 confidence value in order to avoid false positives. 
													nmsThreshold will filter out all boxes that have their IoU threshold less than nmsThreshold. </li>
											</ul>
									</ul>
							</ul>

							<p class = "p_justify">We have developed a user interface that takes in input feed from camera in real-time or recorded video feeds.
								The video frames are fetched and fed to our backend implementation where we process the frame independently 
								using two object detection techniques 1) HOG (using our pre-trained HOG Detector) or 2) YOLO Object Detection.
								 We locate an imaginary trip line at the start of the queue and monitor the location of our detected objects(people) with 
								 respect to this trip line. Whenever a person crosses the trip line, we mark a collision and calculate the throughput time and average 
								 it cumulatively. This tells us real-time efficiency of the queue which can be used as an information to monitor such
								  data for multiple queues concurrently and manage the traffic accordingly.
						   </p>

						<br>

						<br>
						<br>
						<h2 class="title text-center"> <span><strong>Teaser</strong></span></h2>
						
						<div class="space"></div>
								<img src="images/section-image-2.PNG" alt="" class="img-center">
								<div class="space"></div>
						</div>
						
					</div>
				</div>
			</div>
		</section>
		<!-- section end -->
 
		<!-- 
		<div class="default-bg colord secPadding">
			<div class="container">
				<div class="row">
					<div class="col-md-8 col-md-offset-2">
						<h1 class="text-center">Amazing Free Bootstrap Template.</h1>
					</div>
				</div>
			</div>
		</div>
		-->

		<!-- section start --> 
		<section class="section secPadding">
			<div class="container">
				<h1 class="text-center title" id="portfolio">Experiments and Results</h1>
				<br>
				<div class="separator"></div>

				<p class = "p_justify"><strong>Experimentation Setup and Results:</strong> We have kept our main focus on comparing the two techniques with respect to the following
					following parameters. In the end, we will comment based on combination of all the results and their interelation to each other.</p>

				<ul class="list-unstyled">
					<li><i class="fa fa-arrow-circle-right pr-10 colored"></i><strong>Detection Runtime and Output FPS:</strong> </li>
						Our framework calls one of the two detection algorithms based on the option selected on the graphical user interface. We measure the time taken by respective algorithms to process and detect the
						people in the frame. This time is the detection execution time. We have seen a considerable difference in the time taken by YOLO and HoG (data is depicted in the Graph 1). As expected, YOLO requires
						approximately 10x factor as compared to HoG detection time. This affects the FPS of output feed and considerable delay in realtime detection if all frames are fed to the framework.  
				</ul>

				<img src="images/g1g2.PNG" alt="" class="img-center" style="width:600px;height:400px;">
			
	
				
				<ul class="list-unstyled">
					<li><i class="fa fa-arrow-circle-right pr-10 colored"></i><strong>Queue Throughput Time:</strong> </li>
						Our framework constantly detects and monitors the center points of each detected person using either of the algorithms. We have an imaginary tripline at the start of the queue, with a buffer window to account
						for slight consideration of aberrations in the collision of center point of a person with the line. We consider that a person has crossed the line once the center point for that person crosses the line buffer.
						Difference between two collisions is the throughput time for the person that just left. We can maintain a cumulative average of this after every collision. This data helps to know queue efficiency in real-time, 
						which can further be multiplied to the total number of people present in the line, to get waiting time for that queue. Both the algorithms perform equally well in calculating the throughput times as depicted in
						Graph 2(Measured over activity dataset seq26 to showcase).
				</ul>

				
				<ul class="list-unstyled">
					<li><i class="fa fa-arrow-circle-right pr-10 colored"></i><strong>Detection Ratio and Distribution: </strong></li>
						We use the annotations in the activity dataset that have actual data for number of people in the frames as a basis for comparing our results from the two detection algorithms. It is assumed that
						the annotations file given as input is in accordance with the activity data format. Our framework calculates detection ratio as the ratio of people detected in the frame and the number of people in
						the annotations file. This ratio is aggregated over the whole execution which gives us the distribution of these ratios over a period. We have visualized some data for both the algorithms as gaussian curves
						and we notice that for YOLO, the curve is more squeezed with values considerably close to the mean, depicting accuracy and stability. As for HoG, there is high variance in the detection ratio,
						possibily due to intermittent false positives and hence the distribution is more flattened with respect to mean. Thus, we can assume YOLO to have a better accuracy with a trade-off on detection time
						as stated before. The data is visualized in Graph 3(Collected over multiple sequences). 
				</ul>
				<br>
				<br>
				<img src="images/g3.PNG" alt="" class="img-center" style="width:500px;height:400px;">
				<br>
				<br>
				<!--<img src="images/screenshot.png" alt="" class="img-center" style="width:800px;height:550px;">-->
				<h1 class="text-center title" id="conclusion">DEMO</h1>
				<div style="text-align: center;"> <iframe width="700" height="450" src="https://www.youtube.com/embed/T7lv7uTROMs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</div>
				<p class="text-center"><strong>GUI Demo on Activity Dataset Sequence 26</strong></p>
				<div class="space"></div>
				
		</div>
			</div>
		</section>
		<!-- section end -->


		<section class="section transprant-bg pclear secPadding">
			<div class="container no-view" data-animation-effect="fadeIn">
				<h1 id="qualitative" class="title text-center">Qualitative Analysis</h1>
				<ul>
					<li>The first difference that we spotted in the results was the detection of false positives in cases where people crossed in the frames, eclipsing the queue for a short while. As seen from the figure below
						HoG seems to have detected the false positive but the partially eclipsed person in the queue is not detected. As for Yolo, the accuracy is so high that both people are detected. For our application, we assume
						that the camera used for such purpose will be strategically mounted to make sure no such cases come through where the framework has to deal with these kinds of false positives. Such cases will only cause wrong results in
						throughput calculation if the person is moving in the same direction as the queue.
					</li>
						<br>
						<br>
						<img src="images/QA1.PNG" alt="" class="img-center" style="width:800px;height:550px;">
					<br>
					<br>
					<li>In the above images we can also notice that the bounding boxes that are returned from the two algorithms have difference in accuracy of bounding people. for HOG it can be clearly seen that 
						the boxes are less accurately fitting some people where as YOLO is highly accurate with the bounds. A good accuracy in such bounding will give us the best centroid location for the person advanced
						hence the collision time calculated will be more accurate in YOLO.
						We still observe that the throughput accuracy is almost identical for both the algorithms which is due to the fact the people cross the imaginary line relatively quickly, thus the time delta between
						throughputs from two algorithms is similar. This further, makes a case for HoG being useful for this application despite being less accurate in some areas.
					</li>
					
				</ul>
				
			</div>			  
		</section>

		
		<section class="section secPadding">
			<div class="container">
				
				<h1 class="text-center title" id="conclusion">Conclusion</h1>
				Our report compares two well-known and widely used object detection techniques HoG and YOLO for a very specific application for tracking people in a queue and measure throughput for the queues.
				We have dicussed the merits and demerits of both algorithms using our experimental setup parameters mentioned above. Based on the results we observe that despite HoG being a relatively
				less accurate object detection algorithm, it seemed to match YOLO with respect to our use case, however there is a trade-off in runtime. As future scope, we can modify our framework to get the best 
				out of both algorithms with a hybrid model. The idea is to use HoG as the main detection algorithm due to its faster execution time and intermittently(every X frames depending upon the relative movements
				of people within the input image) leverage the accuracy of YOLO to improve on the false positives that are seen in HoG.  
				<br>
				<br>
				<br>
				<h1 class="text-center title" id="ACK">Acknowledgement</h1>
				We want to thank Dr. Lynn Abbott for teaching and providing us with the required skill set to work on Computer Vision at such an intricate level. 
				In addition, the project has enabled us to work on real-world computer vision problems with in-depth knowledge of basic, yet important foundational algorithms and concepts. 
				A special thanks to our peers in the course, who have been part of this short learning journey.
			<br>
			<br>
			<br>
			<div class="container">
				<h1 class="text-center title" id="REF">References</h1>
				<ul class="list-unstyled">
					<li>1) 
						OT. Surasak, I. Takahiro, C. Cheng, C. Wang and P. Sheng, "Histogram of oriented gradients for human detection in video," 2018 5th International Conference on Business and Industrial Research (ICBIR), 2018, pp. 172-176  
					</li>
					<li>
						2) 
						Yussiff AL., Yong SP., Baharudin B.B. (2014) Detecting People Using Histogram of Oriented Gradients: A Step towards Abnormal Human Activity Detection. In: Jeong H., S. Obaidat M., Yen N., Park J. (eds) Advances in Computer Science and its Applications. Lecture Notes in Electrical Engineering, vol 279. Springer, Berlin, Heidelberg.
					</li>
					<li>
						3) 
						S. K. (2019). Comparative study of hog and yolo for traffic sign detection systems (Order No. 13862985). Available From ProQuest Dissertations & Theses Global. 
					</li>
					<li>
						4) 
						P. Ren, W. Fang and S. Djahel, "A novel YOLO-Based real-time people counting approach," 2017 International Smart Cities Conference (ISC2), 2017, pp. 1-2, doi: 10.1109/ISC2.2017.8090864.
					</li>
					<li>
						5) 
						W. Lan, J. Dang, Y. Wang and S. Wang, "Pedestrian Detection Based on YOLO Network Model," 2018 IEEE International Conference on Mechatronics and Automation (ICMA), 2018, pp. 1547-1551, doi: 10.1109/ICMA.2018.8484698.
					</li>
					<li>
						6) 
						S. Manzoor, S. Joo and T. Kuc, "Comparison of Object Recognition Approaches using Traditional Machine Vision and Modern Deep Learning Techniques for Mobile Robot," 2019 19th International Conference on Control, Automation and Systems (ICCAS), 2019, pp. 1316-1321
					</li>
				</ul>
		</section>

		<footer>
			
			<div class="subfooter">
				<div class="container">
					<div class="row">
						<div class="col-md-12">
							<p class="text-center">Copyright © <a target="_blank" href="https://www.linkedin.com/in/kriti-kansal-92a48411b/">Kriti Kansal</a> and <a target="_blank" href="https://www.linkedin.com/in/jain08/">Shashwat Jain</a></p>
						</div>
					</div>
				</div>
			</div>
			

		</footer>
		

		 
		<script type="text/javascript" src="plugins/jquery.min.js"></script>
		<script type="text/javascript" src="bootstrap/js/bootstrap.min.js"></script>
		<script type="text/javascript" src="plugins/modernizr.js"></script>
		<script type="text/javascript" src="plugins/isotope/isotope.pkgd.min.js"></script>
		<script type="text/javascript" src="plugins/jquery.backstretch.min.js"></script>
		<script type="text/javascript" src="plugins/jquery.appear.js"></script>

		 
		<script type="text/javascript" src="js/custom.js"></script>
	</body>
</html>
